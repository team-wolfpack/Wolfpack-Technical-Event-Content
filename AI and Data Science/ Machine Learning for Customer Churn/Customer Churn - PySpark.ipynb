{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>IBM WolfPack</center>\n",
    "\n",
    "<img style=\"float: center;\" src=\"https://github.com/team-wolfpack/Predicting-Customer-Churn-with-Watson-Data-Platform/blob/master/Documents/Team%20%23WolfPack-01.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><span style=\"color:blue\">Predicting Customer Churn with Watson Data Platform </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adapted from \"Predict Customer Churn Use Case Implementation\" by Sydney Phoon | https://github.com/SidneyPhoon/IntroToNotebooksLab*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Step 1: Download the customer churn data](#download)<br/>\n",
    "2. [Step 2: Read data into Spark DataFrames](#getdata)<br/>\n",
    "3. [Step 3: Merge Files](#merge)<br/>\n",
    "4. [Step 4: Rename some columns](#rename)<br/>\n",
    "5. [Step 5: Data understanding](#dataunderstanding)<br/>\n",
    "    5.1 [Dataset overview](#overview)<br/>\n",
    "    5.2 [VectorAssembler](#vectorassembler)<br/>\n",
    "    5.3 [Normalizer](#normalizer)<br/>\n",
    "7. [Step 7: Applying Spark pipeline concepts to customer churn data](#applypipelineconcepts)<br/>\n",
    "8. [Step 8: Creating a Spark ML pipeline](#createpipeline)<br/>\n",
    "9. [Step 9: Score the test dataset](#scoretestdata)<br/>\n",
    "10. [Step 10: Model evaluation](#evaluate)<br/>\n",
    "11. [Step 11: Execute inline invocation of the Random Forests Model](#execute)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"download\"></a>\n",
    "# <span style=\"color:#fa04d9\"> Step 1: Download the customer churn data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once to install the wget package\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from GitHub repository\n",
    "import wget\n",
    "url_churn='https://raw.githubusercontent.com/team-wolfpack/Predicting-Customer-Churn-with-Watson-Data-Platform/master/Data/churn.csv'\n",
    "url_customer='https://raw.githubusercontent.com/team-wolfpack/Predicting-Customer-Churn-with-Watson-Data-Platform/master/Data/customer.csv'\n",
    "    \n",
    "#remove existing files before downloading\n",
    "!rm -f churn.csv\n",
    "!rm -f customer.csv\n",
    "\n",
    "churnFilename=wget.download(url_churn)\n",
    "customerFilename=wget.download(url_customer)\n",
    "\n",
    "#list existing files\n",
    "!ls -l churn.csv\n",
    "!ls -l customer.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"getdata\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 2: Read data into Spark DataFrames</span>\n",
    "\n",
    "Note: You want to reference the Spark DataFrame API to learn more about the supported operations, https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "churn_df= spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"churn.csv\")\n",
    "\n",
    "customer_df = spark.read\\\n",
    "    .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take a look at the 5 first datapoints from the newly loaded Spark dataframes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the schema\n",
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 3: Merge Files </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = customer_df.join(churn_df,customer_df['ID'] == churn_df['ID']).select(customer_df['*'], churn_df['CHURN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rename\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 4: Rename some columns </span>\n",
    "This step is not a requirement, it just makes some columns names simpler to type with no spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumnRenamed renames an existing column in a SparkDataFrame and returns a new SparkDataFrame\n",
    "data_df = data_df.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n",
    "data_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataunderstanding\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 5: Data understanding </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dataset Overview\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Pandas library has a powerful set commands to analyze data. As an example, check the use of \"describe\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = data_df.toPandas()\n",
    "print \"There are \" + str(len(df_pandas)) + \" observations in the customer history dataset.\"\n",
    "print \"There are \" + str(len(df_pandas.columns)) + \" variables in the dataset.\"\n",
    "\n",
    "print \"\\n******************Descriptive statistics*****************************\\n\"\n",
    "print df_pandas.drop(['ID'], axis = 1).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Brunel** Visualization library provides a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and more aggressive business users. The system interprets the language and produces visualizations using the user's choice of existing lower-level visualization technologies typically used by application engineers such as RAVE or D3. \n",
    "\n",
    "More information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n",
    "\n",
    "Try Brunel visualization here:  http://brunel.mybluemix.net/gallery_app/renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import brunel\n",
    "df_pandas = data_df.toPandas()\n",
    "%brunel data('df_pandas') stack bar x(Paymethod) y(#count) color(CHURN) bin(Paymethod) percent(#count) label(#count) tooltip(#all) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an interactive UI in which you can explore data.<br/>\n",
    "More information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you haven't already installed it, uncomment and run the following cell to install the pixiedust Python library in your notebook environment. You only need to run it once**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "50",
      "charttype": "grouped",
      "clusterby": "CHURN",
      "handlerId": "barChart",
      "keyFields": "Paymethod",
      "mpld3": "false",
      "orientation": "vertical",
      "rowCount": "500",
      "timeseries": "false",
      "valueFields": "Usage"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pixiedust.display import *\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparksql\"></a>\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Interactive query with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL also allow you to use standard SQL\n",
    "data_df.createOrReplaceTempView(\"data_df\")\n",
    "sql = \"\"\"\n",
    "SELECT c.*\n",
    "FROM data_df c\n",
    "WHERE c.EstIncome>50000\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intropipeline\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 6: Introduction to Spark Pipelines (Optional. if you are already familiar with these concepts, please skip to Step 7).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the section following this one, you will be building a SparkML Pipeline which consists of Transformers and Estimators. As a preamble to that section, users who are not familiar with the concepts and terminology of \"Transformers\", \"Estimators\" and \"Pipeline\" are invited to take advantage of this section to get familiarity with those concepts. Users who are already familiar with these concepts can skip directly to the next section of this notebook: Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this section, you will get familiar with a few important Spark ML concepts:\n",
    "### <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Discovering some Estimators, Transformers and what they do.\n",
    "### <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Introduction to the notion of a Spark Machine Learning Pipeline.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stringindexer\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Estimator: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#stringindexer\">StringIndexer</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0.<br><br> Note that StringIndexer is an estimator, not a transformer. StringIndexer needs to scan the data it is given as input, to find the most frequent string and assign to it label 0, and then label 1 to the next most frequent string and so on. It will then produce a StringIndexerModel which is a transformer which can be applied to the input data using the \"transform\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-10\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-10\" href=\"#collapse1-10\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that new cell to see how StringIndexer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-10\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.feature import StringIndexer<br>\n",
    "<br>\n",
    "df = spark.createDataFrame( <br>\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")], <br>\n",
    "    [\"id\", \"category\"]) <br>\n",
    "<br>\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") <br>\n",
    "indexed = indexer.fit(df).transform(df) <br>\n",
    "indexed.show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"indextostring\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#indextostring\">IndexToString</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrically to StringIndexer, IndexToString maps a column of label indices back to a column containing the original labels as strings. A common use case is to produce indices from labels with StringIndexer, train a model with those indices and retrieve the original labels from the column of predicted indices with IndexToString. However, you are free to supply your own labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"onehotencoder\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#onehotencoder\">OneHotEncoder</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous (quantitative to be precise as the output is discrete) features, such as Logistic Regression, to use categorical features. OneHotEncoder is a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bucketizer\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#bucketizer\">Bucketizer</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter defining the number of buckets. Bucketizing data is also referred to as \"binning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vectorassembler\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\">VectorAssembler</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalizer\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#normalizer\">Normalizer</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">There are several other Estimators and Transformers which are documented in the Apache documentation online right <a href=\"https://spark.apache.org/docs/latest/ml-features.html\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"applypipelineconcepts\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 7: Applying the concepts described above to our customer churn dataset: \n",
    "** * a) Defining and applying the StringIndexer Estimator to input columns Gender, Status, CarOwner, Paymethod, LocalBilltype, LongDistanceBilltype. **<br>\n",
    "** * b) Defining and applying VectorAssembler to the columns above to group them as one input vector to the model. **<br>\n",
    "** * c) Defining and applying a StringIndexer Estimator to the target label column \"CHURN\", to encode the T/F values into 0/1. ** <br>\n",
    "** * d) Defining and applying an IndexToString Transformer to reverse the output of our model from 0/1 predictions back to T/F values . ** <br>\n",
    "** * e) Defining the Random Forest estimator itself, which will be trained on the input training data to produce the actual model which will perform the predictions. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) Defining a StringIndexer for the String columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this dataset, we will encode columns Gender, Status, CarOwner, Paymethod, LocalBilltype and LongDistanceBilltype\n",
    "# StringIndexer encodes a string column of labels to a column of label indices. \n",
    "SI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\n",
    "SI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\n",
    "SI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\n",
    "SI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\n",
    "SI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\n",
    "SI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) Let's see what StringIndexer is actually doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = SI4.fit(data_df).transform(data_df)\n",
    "indexed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL also allow you to use standard SQL\n",
    "indexed.createOrReplaceTempView(\"indexed\")\n",
    "sql = \"\"\"\n",
    "SELECT distinct Paymethod, PaymethodEncoded, count(1)\n",
    "FROM indexed c\n",
    "group by c.Paymethod, PaymethodEncoded\n",
    "order by 3 desc\n",
    "\"\"\"\n",
    "spark.sql(sql).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c) Define a Vector Assembler for all the columns of interest to be passed into the chosen machine learning model (columns which are encoded as well as those kept as is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \\\n",
    "                                       \"LocalBilltypeEncoded\", \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \\\n",
    "                                       \"LongDistance\", \"International\", \"Local\", \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d) Defining a StringIndexer for the label column of our model (CHURN column. The values True and False will be converted to 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the label column\n",
    "labelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e) Defining an IndexToString transformer to bring the labels back to True and False once the predictions are done. The model will produce a column named \"prediction\" which will contain 0 or 1. We will convert it back to True and False in a column named \"predictedLabel.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f) Defining a Random Forest estimator, and a Decision Tree (CART) estimator. These are two very popular tree based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the algorithms, take the default settings\n",
    "# Random Forests\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "# Classification and Regression Trees (CART)\n",
    "dt=DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"createpipeline\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 8: Creating a Spark ML pipeline:\n",
    "** * All the individual components of the pipeline have been defined in the section above. Notice how we will now \"group\" them into a pipeline object.</span> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
    "* Split each document’s text into words. \n",
    "* Convert each document’s words into a numerical feature vector.  \n",
    "* Learn a prediction model using the feature vectors and labels.<br>\n",
    "\n",
    "### MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the Spark pipeline including the operations defined in Step 7 above.\n",
    "\"Pipeline\" is an API in SparkML. A pipeline defines a sequence of transformers and estimators to perform the analysis in stages.\n",
    "Additional information on SparkML is available online, including at this link: https://spark.apache.org/docs/2.0.2/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipeline\n",
    "# Random Forests\n",
    "rfPipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])\n",
    "\n",
    "# Classification and Regression Trees (CART)\n",
    "dtPipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, dt, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into Training and Testing sets (this is a standard best practice in data science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "(trainingData, testingData) = data_df.randomSplit([0.7, 0.3],seed=9)\n",
    "trainingData.cache()\n",
    "testingData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model from fitting the whole pipeline using the training data set. <br><br>Note that the pipeline interface will correctly call fit+transform or just transform alone for each stage of the pipeline, depending on whether the current stage is an estimator (such as StringIndex) or a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forests model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "rfModel =rfPipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Decision Trees model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "dtModel =dtPipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"scoretestdata\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 9: Score the test data set </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rfResult=rfModel.transform(testingData)\n",
    "rfResultDisplay=rfResult.select(rfResult[\"ID\"],rfResult[\"CHURN\"],rfResult[\"Label\"],rfResult[\"predictedLabel\"],rfResult[\"prediction\"],rfResult[\"probability\"])\n",
    "rfResultDisplay.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "dtResult=dtModel.transform(testingData)\n",
    "dtResultDisplay=dtResult.select(dtResult[\"ID\"],dtResult[\"CHURN\"],dtResult[\"Label\"],dtResult[\"predictedLabel\"],dtResult[\"prediction\"],dtResult[\"probability\"])\n",
    "dtResultDisplay.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 10: Model Evaluation </span>\n",
    "** Find accuracy of the model and the Area Under the ROC Curve **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "print ' Random Forests Model Accuracy = {:.2f}.'.format(rfResult.filter(rfResult.label == rfResult.prediction).count() / float(rfResult.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "print ' Decision Trees Model Accuracy = {:.2f}.'.format(dtResult.filter(dtResult.label == dtResult.prediction).count() / float(dtResult.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\n",
    "Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n",
    "\n",
    "Additional reading on this metric can be found very easily online, such as at this wikipedia link: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate Random Forests model\n",
    "rfEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Random Forests: Area under ROC curve = {:.3f}.'.format(rfEvaluator.evaluate(rfResult))\n",
    "\n",
    "# Evaluate Decision Trees model\n",
    "dtEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Decistion Tress: Area under ROC curve = {:.3f}.'.format(dtEvaluator.evaluate(dtResult))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"execute\"></a>\n",
    "# <span style=\"color:#fa04d9\">Step 11: Execute an inline invocation of the Random Forest Model.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now make a prediction on some customer for which we will provide our own made up attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = 'F'\n",
    "Status = 'M'\n",
    "CarOwner = 'N'\n",
    "Paymethod = 'CC'\n",
    "LocalBilltype = 'Budget'\n",
    "LongDistanceBilltype = 'Standard'\n",
    "Children = 1\n",
    "EstIncome = 45000\n",
    "Age = 30\n",
    "LongDistance = 30\n",
    "International = 0\n",
    "Local = 100\n",
    "Dropped = 0\n",
    "Usage = 150\n",
    "RatePlan = 2\n",
    "\n",
    "Features = (spark.createDataFrame([(Gender, Status, CarOwner, Paymethod, LocalBilltype, LongDistanceBilltype, Children, EstIncome, Age, LongDistance, \\\n",
    "                                              International, Local, Dropped, Usage, RatePlan)],\n",
    "    ['Gender', 'Status', 'CarOwner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype', 'Children', 'EstIncome', 'Age', 'LongDistance', \\\n",
    "     'International', 'Local', 'Dropped', 'Usage', 'RatePlan']))\n",
    "Features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChurnPrediction = rfModel.transform(Features)\n",
    "ChurnPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Exercise: Change the number of children and/or the EstIncome in the cell prior to the one above, and observe the impact on the prediction:\n",
    "* It seems that a number of children lower than 3 will result in churn, but a customer with 3 children or more will not churn.\n",
    "* The rule above is true for lower incomes. With higher incomes, churn is less likely (if we change the income to 145,000 the model does not seem to predict churn anymore, regardless of the number of children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Spark Lab Excercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
